#!/usr/bin/env python3
import argparse
import json
import pathlib
from paperboy_fetcher.fetchers import http
from paperboy_fetcher.rss import nice_parser
from paperboy_fetcher.rss import CNN as cnn_parser
from paperboy_fetcher.extractors import CNN, BBC, NYT, WAPO
import hashlib

arg_parser = argparse.ArgumentParser(description='RSS scanner that fetches and extracts article text to feed name directories')
arg_parser.add_argument('--config', type=str, help='the location of the config file')
arg_parser.add_argument('--output', type=str, help='the output directory to store articles in')


def get_rss_parser(type):
    if type == 'CNN':
        return cnn_parser
    else:
        return nice_parser


def get_extractor(type):
    if type == 'CNN':
        return CNN
    if type == 'BBC':
        return BBC
    if type == 'WAPO':
        return WAPO
    if type == 'NYT':
        return NYT


def download_feed_articles(cwd, type, url):
    rss_parser = get_rss_parser(type)
    extractor = get_extractor(type)
    articles = rss_parser.extract_articles(url)
    for article in articles:
        link = article['link']
        content = http.fetch(link)
        extracted = None
        try:
            extracted = extractor.extract_article(content)
            extracted['link'] = article['link']  # augment with the link so we have that data available in the ML layer
        except:
            print("Unable to parse article from " + type + " with link " + link)
        if extracted is not None:
            print("Saved article from " + type + " with link " + link)
            save_article_file(cwd, extracted)


def save_article_file(cwd, article):
    content = article['article']
    encoded = content.encode('utf-8')
    hasher = hashlib.md5()
    hasher.update(encoded)
    hash = hasher.digest()
    hex = hash.hex()
    filename = hex + ".json"
    file_path = cwd.joinpath(filename)
    with open(file_path, 'w') as file:
        json.dump(article, file)


def fetch_feeds(cwd, feed_provider, feed_list):
    for feed in feed_list:
        name = feed['name']
        parts = [feed_provider, name]
        feed_path = cwd.joinpath(*parts)
        feed_path.mkdir(parents=True, exist_ok=True)
        download_feed_articles(feed_path, feed['type'], feed['url'])


if __name__ == '__main__': # which it will:
    args = arg_parser.parse_args()
    args_dict = vars(args)
    # parse the config file
    config_location = args_dict.get('config', './rss_config.json')
    output_dir = args_dict.get('output', './articles')
    with open(config_location, 'r') as config_file:
        config_data = config_file.read()
        config = json.loads(config_data)

    # now make the output dir
    output_path = pathlib.Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    feeds = config['feeds']
    for name, feed_list in feeds.items():
        fetch_feeds(output_path, name, feed_list)

